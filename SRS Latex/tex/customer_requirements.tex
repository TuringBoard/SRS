Specific requirements for the Turing Board were established with the enjoyment and convenience of the user in mind. In order to have an electronic longboard with autonomous capabilities the majority of the customer requirements are based on functionality. Users will be able to propel the board forward and backward using a mobile application to adjust the speed of electric motors embedded into the trucks. For safety, all autonomous features will be disabled while a rider is on the board. To determine if a rider is present a weight sensor will be included on the Turing Board. While in autonomous mode the user can expect the board to know and remember its location using GPS. While within a limited range the user will be able to summon the board to their location. The user will also be able to enable a follow mode that will keep the board within close proximity while the user is mobile. Both summon and follow features will need computer vision and path finding algorithms to detect and avoid any object.The board must also be able to turn without the assistance of a riders weight compressing the bushings within the trucks. To accomplish this the longboard will have a 14mm turning mechanism sit between the front trucks and the board, allowing forty-five degree rotation in either direction. The following customer requirements were developed to ensure the user's experience matches their expectation's of an autonomous longboard.

\subsection{Forward Propulsion System}
\subsubsection{Description}
Two six-hundred watt brush less DC motor's are embedded into the rear truck and will be powered through the on board battery. Utilizing an electronic speed controller the motors will be driven to a specific RPM that corresponds to the users desired speed.
\subsubsection{Source}
This requirement comes from the user and is the key component in making this an electronic longboard.
\subsubsection{Constraints}
The forward propulsion system must use a control loop that takes environmental factors into account and adjusts the torque to accommodate any change in variables. Within reason the user should expect to go the same speed regardless of body weight or slight changes to terrain.
\subsubsection{Standards}
Conforms to the Texas state motor-assisted scooter laws.
\subsubsection{Priority}
Critical

\subsection{GPS}
\subsubsection{Description}
As part of the autonomous navigation system, the GPS would provide coordinate data which would allow the program to know the position of the longboard which would be used to draw vectors to aid with navigation to a target coordinate.
\subsubsection{Source}
The GPS built into the phone will be used to get the current location of the longboard.
\subsubsection{Constraints}
The GPS coordinates provided by the phone should be precise without jumping from one value to another too much.
\subsubsection{Standards}
Conforms to the standard Global Positioning System.
\subsubsection{Priority}
High

\subsection{Remote}
\subsubsection{Description}
Instead of having a separate remote, a free-to-download app will be made available which can be used to summon the board, enable the follow-me feature, and control the speed of the wheels. The app currently uses React as the framework of choice for the front-end and is hosted publicly on Netlify. It uses sockets for communication, but a better approach we found is Bluetooth which would make the process of data transfer very simple and easy. The app  will soon be changed to using React Native to support both Android and iOS. The reason for this is to ensure full compatibility without having to worry about browser versions.
\subsubsection{Source}
Native Android and iOS application making use of Bluetooth to communicate with the longboard.
\subsubsection{Constraints}
Since the rider of the longboard will be going quite fast, the transmission of data needs to be reduced as much as possible. Latency should be minimized.
\subsubsection{Standards}
Bluetooth 4.0/5.0 for maximum compatibility.
\subsubsection{Priority}
High

\subsection{Computer Vision}
\subsubsection{Description}
The Turing Board utilizes an Intel RealSense Depth Camera D435. It has two that will be used for detecting objects to avoid and detecting a specific maker in its view to command the board to follow (see "Other" section for more information on this feature). The object detection is handled by Single Shot Detectors (SSDs). An SSD-MobileNet V2 model on a 91 class COCO dataset will be deployed.
\subsubsection{Source}
Sahaj Amatya & Sarker Nadir Afridi Azmi
\subsubsection{Constraints}
The pre-built library for the computer vision only works on x86 architecture. Since our controller, the NVIDIA Jetson TX 2, was built to work with Intel ARM architecture a kernel has been built to allow communication between the two.
Also, due to the nature of what the computer vision of the Turing Board will do, the Jetson TX 2 will need to use it's CUDA cores to process the input appropriately. For this reason, a kernel was also built for OpenCV.
\subsubsection{Standards}
N/A
\subsubsection{Priority}
High

\subsection{Path finding}
\subsubsection{Description}
The Turing Board will utilize a greedy best first search for its path finding algorithm. The board generates a trapezoidal trajectory map in front of it with progressive width equal to its own width plus some padding width. If an object is detected with the trajectory, the path finding algorithm will seek a path to avoid the object.
\subsubsection{Source}
Sahaj Amatya \& Sarker Nadir Afridi Azmi
\subsubsection{Constraints}
The camera used on the board is able to only see up to 20ft ahead. This will need to be taken into consideration with considering how to best avoid an object.
\subsubsection{Standards}
N/A
\subsubsection{Priority}
High
